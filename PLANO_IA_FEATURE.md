# Plano de Implementa√ß√£o: Feature de IA Multi-Provedor para Gera√ß√£o de Textos

## Vis√£o Geral

Implementar sistema que permite usar m√∫ltiplos provedores de IA (OpenAI, Google Gemini, Anthropic, e outros) para gerar trechos de texto automaticamente em documentos. O usu√°rio define tags no formato `{{ai:paragrapho1}}` no template do Google Docs e configura no workflow quais campos do HubSpot ser√£o usados para gerar cada texto, al√©m de escolher o provedor e modelo de IA.

---

## Arquitetura: LiteLLM como Wrapper Unificado ‚úÖ

> **Decis√£o**: Usar **LiteLLM** como √∫nica biblioteca de IA. N√£o ser√£o criadas classes separadas por provedor (OpenAIService, GeminiService, etc). LiteLLM abstrai 100+ modelos com interface √∫nica.

### Benef√≠cios:
- Menos c√≥digo para manter
- Interface unificada para todos os provedores
- Suporte autom√°tico a novos modelos
- Retry logic e error handling embutidos
- Formato de modelo: `provider/model` (ex: `openai/gpt-4`, `gemini/gemini-1.5-pro`)

---

## Componentes Principais

### 1. Servi√ßo de IA Unificado (LiteLLM)

- **Arquivo**: `app/services/ai/llm_service.py` (novo)
- Classe `LLMService` que encapsula LiteLLM
- Interface √∫nica para todos os provedores

```python
class LLMService:
    def generate_text(
        self,
        model: str,           # Ex: "openai/gpt-4", "gemini/gemini-1.5-pro"
        prompt: str,
        api_key: str,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        timeout: int = 60,
        **kwargs
    ) -> LLMResponse:
        """Gera texto usando LiteLLM"""
        
    def validate_api_key(
        self,
        provider: str,
        api_key: str
    ) -> bool:
        """Valida se API key √© v√°lida fazendo chamada teste"""
```

### 2. Utilit√°rios e Helpers

- **Arquivo**: `app/services/ai/utils.py` (novo)
- Fun√ß√µes para validar modelos suportados via LiteLLM
- Normalizar nomes de provedores
- Listar provedores e modelos dispon√≠veis
- Validar formato de modelo
- Helper para custo estimado por modelo

```python
SUPPORTED_PROVIDERS = ['openai', 'gemini', 'anthropic']
PROVIDER_MODELS = {
    'openai': ['gpt-4', 'gpt-4-turbo', 'gpt-3.5-turbo'],
    'gemini': ['gemini-1.5-pro', 'gemini-1.5-flash'],
    'anthropic': ['claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku']
}

def get_model_string(provider: str, model: str) -> str:
    """Retorna string de modelo para LiteLLM (ex: openai/gpt-4)"""
    
def get_available_providers() -> list:
    """Lista provedores dispon√≠veis"""
    
def get_available_models(provider: str) -> list:
    """Lista modelos dispon√≠veis para um provedor"""
    
def estimate_cost(provider: str, model: str, tokens: int) -> float:
    """Estima custo baseado em tokens"""
```

### 3. Modelo de Dados: AIGenerationMapping

- **Arquivo**: `app/models/workflow.py`
- Criar modelo `AIGenerationMapping` similar a `WorkflowFieldMapping`

```python
class AIGenerationMapping(db.Model):
    __tablename__ = 'ai_generation_mappings'
    
    id = db.Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    workflow_id = db.Column(UUID(as_uuid=True), db.ForeignKey('workflows.id'), nullable=False)
    
    # Tag e configura√ß√£o
    ai_tag = db.Column(db.String(255), nullable=False)  # Ex: "paragrapho1"
    source_fields = db.Column(JSONB)  # Array de campos HubSpot
    
    # Provedor e modelo
    provider = db.Column(db.String(50), nullable=False)  # 'openai', 'gemini', etc
    model = db.Column(db.String(100), nullable=False)    # 'gpt-4', 'gemini-pro', etc
    ai_connection_id = db.Column(UUID(as_uuid=True), db.ForeignKey('data_source_connections.id'))
    
    # Configura√ß√£o do prompt
    prompt_template = db.Column(db.Text)  # Template com placeholders {{field}}
    temperature = db.Column(db.Float, default=0.7)
    max_tokens = db.Column(db.Integer, default=1000)
    
    # Fallback (se IA falhar)
    fallback_value = db.Column(db.Text)  # Valor padr√£o se gera√ß√£o falhar
    
    # M√©tricas de uso (para auditoria/debugging)
    last_used_at = db.Column(db.DateTime)
    usage_count = db.Column(db.Integer, default=0)
    
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # √çndices
    __table_args__ = (
        db.UniqueConstraint('workflow_id', 'ai_tag', name='unique_workflow_ai_tag'),
        db.Index('idx_ai_mapping_connection', 'ai_connection_id'),
    )
    
    # Relationships
    ai_connection = db.relationship('DataSourceConnection', foreign_keys=[ai_connection_id])
    
    def to_dict(self):
        return {
            'id': str(self.id),
            'workflow_id': str(self.workflow_id),
            'ai_tag': self.ai_tag,
            'source_fields': self.source_fields,
            'provider': self.provider,
            'model': self.model,
            'ai_connection_id': str(self.ai_connection_id) if self.ai_connection_id else None,
            'prompt_template': self.prompt_template,
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'fallback_value': self.fallback_value,
            'last_used_at': self.last_used_at.isoformat() if self.last_used_at else None,
            'usage_count': self.usage_count,
            'created_at': self.created_at.isoformat() if self.created_at else None,
            'updated_at': self.updated_at.isoformat() if self.updated_at else None
        }
```

### 4. Atualiza√ß√£o do Modelo Workflow

- **Arquivo**: `app/models/workflow.py`
- Adicionar relationship para ai_mappings

```python
class Workflow(db.Model):
    # ... campos existentes ...
    
    # Adicionar este relationship:
    ai_mappings = db.relationship(
        'AIGenerationMapping', 
        backref='workflow', 
        lazy='dynamic', 
        cascade='all, delete-orphan'
    )
    
    def to_dict(self, include_mappings=False, include_ai_mappings=False):
        result = {
            # ... campos existentes ...
        }
        
        if include_mappings:
            result['field_mappings'] = [m.to_dict() for m in self.field_mappings]
        
        # Adicionar:
        if include_ai_mappings:
            result['ai_mappings'] = [m.to_dict() for m in self.ai_mappings]
        
        return result
```

### 5. Atualiza√ß√£o do Models __init__.py

- **Arquivo**: `app/models/__init__.py`
- Exportar o novo modelo

```python
from .workflow import Workflow, WorkflowFieldMapping, AIGenerationMapping

__all__ = [
    # ... existentes ...
    'AIGenerationMapping',
]
```

### 6. Armazenamento de API Keys (BYOK)

- **Arquivo**: `app/models/connection.py`
- Usar `DataSourceConnection` existente para armazenar credenciais de IA
- Tipos suportados: 'openai', 'gemini', 'anthropic'
- Usar criptografia existente (`app/utils/encryption.py`)

> **Nota**: N√£o √© necess√°rio criar AIDataSource seguindo o padr√£o BaseDataSource, pois os provedores de IA n√£o s√£o fontes de dados no sentido tradicional (n√£o puxamos dados deles, apenas enviamos prompts).

### 7. Processamento de Tags AI

- **Arquivo**: `app/services/document_generation/tag_processor.py`
- Estender `TagProcessor` para detectar tags `{{ai:...}}`

```python
class TagProcessor:
    AI_TAG_PATTERN = r'\{\{ai:([^}]+)\}\}'
    
    def extract_ai_tags(self, content: str) -> list[str]:
        """Retorna lista de nomes de tags AI encontradas"""
        matches = re.findall(self.AI_TAG_PATTERN, content)
        return list(set(matches))
    
    async def process_ai_tag(
        self,
        tag_name: str,
        mapping: AIGenerationMapping,
        source_data: dict,
        llm_service: LLMService
    ) -> str:
        """
        Processa uma tag AI:
        1. Coleta valores dos campos mapeados
        2. Monta prompt usando template
        3. Chama servi√ßo de IA
        4. Retorna texto gerado ou fallback
        """
```

### 8. Integra√ß√£o no Fluxo de Gera√ß√£o

- **Arquivo**: `app/services/document_generation/generator.py`
- Modificar `DocumentGenerator.generate_from_workflow()`:

```python
async def generate_from_workflow(self, workflow, source_data: dict) -> GeneratedDocument:
    # 1. Buscar template
    template_content = self.get_template_content(workflow.template)
    
    # 2. Detectar tags AI no template
    ai_tags = self.tag_processor.extract_ai_tags(template_content)
    
    # 3. Processar tags AI primeiro
    ai_metrics = AIGenerationMetrics()
    for tag_name in ai_tags:
        mapping = self.get_ai_mapping(workflow, tag_name)
        if mapping:
            start_time = time.time()
            try:
                generated_text = await self.process_ai_tag(mapping, source_data)
                template_content = self.replace_tag(template_content, f"ai:{tag_name}", generated_text)
                ai_metrics.add_success(mapping, time.time() - start_time)
            except AIGenerationError as e:
                # Usar fallback ou manter tag
                fallback = mapping.fallback_value or f"[Erro: {tag_name}]"
                template_content = self.replace_tag(template_content, f"ai:{tag_name}", fallback)
                ai_metrics.add_failure(mapping, str(e))
    
    # 4. Processar tags normais
    # ... c√≥digo existente ...
    
    # 5. Salvar m√©tricas de IA no execution
    self.save_ai_metrics(execution, ai_metrics)
```

### 9. Integra√ß√£o com WorkflowExecution

- **Arquivo**: `app/models/execution.py`
- Adicionar campo para m√©tricas de IA

```python
class WorkflowExecution(db.Model):
    # ... campos existentes ...
    
    # Adicionar:
    ai_metrics = db.Column(JSONB)  # M√©tricas de gera√ß√£o de IA
    
    # Estrutura do ai_metrics:
    # {
    #     "total_tags": 3,
    #     "successful": 2,
    #     "failed": 1,
    #     "total_time_ms": 4500,
    #     "total_tokens": 1200,
    #     "estimated_cost_usd": 0.024,
    #     "details": [
    #         {
    #             "tag": "paragrapho1",
    #             "provider": "openai",
    #             "model": "gpt-4",
    #             "time_ms": 2100,
    #             "tokens": 800,
    #             "status": "success"
    #         },
    #         ...
    #     ]
    # }
```

---

## Rotas/Endpoints

### Conex√µes de IA (em `app/routes/connections.py`)

```python
# Criar/atualizar conex√£o de IA
POST /api/v1/connections/ai
Body: { 
    "organization_id": "uuid",
    "provider": "openai",  # openai, gemini, anthropic
    "api_key": "sk-...",
    "name": "OpenAI Principal"  # opcional
}

# Listar conex√µes de IA da organiza√ß√£o
GET /api/v1/organizations/<org_id>/connections/ai

# Obter conex√£o espec√≠fica
GET /api/v1/connections/ai/<connection_id>

# Atualizar conex√£o
PATCH /api/v1/connections/ai/<connection_id>
Body: { "api_key": "sk-new...", "name": "Novo nome" }

# Deletar conex√£o
DELETE /api/v1/connections/ai/<connection_id>

# ‚ö†Ô∏è NOVO: Testar conex√£o (validar API key)
POST /api/v1/connections/ai/<connection_id>/test
Response: { "valid": true, "provider": "openai", "message": "API key v√°lida" }
```

### Mapeamentos de IA (em `app/routes/workflows.py`)

```python
# Criar mapeamento de IA
POST /api/v1/workflows/<workflow_id>/ai-mappings
Body: {
    "ai_tag": "paragrapho1",
    "source_fields": ["dealname", "amount", "company.name"],
    "provider": "openai",
    "model": "gpt-4",
    "ai_connection_id": "uuid",
    "prompt_template": "Gere um par√°grafo descrevendo o deal {{dealname}} no valor de {{amount}} para a empresa {{company.name}}",
    "temperature": 0.7,
    "max_tokens": 500,
    "fallback_value": "[Texto n√£o gerado]"
}

# Listar mapeamentos do workflow
GET /api/v1/workflows/<workflow_id>/ai-mappings

# Obter mapeamento espec√≠fico
GET /api/v1/workflows/<workflow_id>/ai-mappings/<mapping_id>

# Atualizar mapeamento
PATCH /api/v1/workflows/<workflow_id>/ai-mappings/<mapping_id>

# Deletar mapeamento
DELETE /api/v1/workflows/<workflow_id>/ai-mappings/<mapping_id>
```

### Rotas Auxiliares de IA (em `app/routes/ai_routes.py` - novo)

```python
# Listar provedores dispon√≠veis
GET /api/v1/ai/providers
Response: [
    { "id": "openai", "name": "OpenAI", "models": ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"] },
    { "id": "gemini", "name": "Google Gemini", "models": ["gemini-1.5-pro", "gemini-1.5-flash"] },
    { "id": "anthropic", "name": "Anthropic", "models": ["claude-3-opus", "claude-3-sonnet"] }
]

# Listar modelos de um provedor
GET /api/v1/ai/providers/<provider>/models
Response: ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"]
```

---

## Migra√ß√£o de Banco de Dados

- **Arquivo**: `migrations/versions/xxx_add_ai_generation_tables.py`

```python
def upgrade():
    # Criar tabela ai_generation_mappings
    op.create_table(
        'ai_generation_mappings',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True),
        sa.Column('workflow_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('workflows.id'), nullable=False),
        sa.Column('ai_tag', sa.String(255), nullable=False),
        sa.Column('source_fields', postgresql.JSONB),
        sa.Column('provider', sa.String(50), nullable=False),
        sa.Column('model', sa.String(100), nullable=False),
        sa.Column('ai_connection_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('data_source_connections.id')),
        sa.Column('prompt_template', sa.Text),
        sa.Column('temperature', sa.Float, default=0.7),
        sa.Column('max_tokens', sa.Integer, default=1000),
        sa.Column('fallback_value', sa.Text),
        sa.Column('last_used_at', sa.DateTime),
        sa.Column('usage_count', sa.Integer, default=0),
        sa.Column('created_at', sa.DateTime, default=datetime.utcnow),
        sa.Column('updated_at', sa.DateTime, default=datetime.utcnow),
    )
    
    # √çndices
    op.create_unique_constraint('unique_workflow_ai_tag', 'ai_generation_mappings', ['workflow_id', 'ai_tag'])
    op.create_index('idx_ai_mapping_connection', 'ai_generation_mappings', ['ai_connection_id'])
    
    # Adicionar coluna ai_metrics em workflow_executions
    op.add_column('workflow_executions', sa.Column('ai_metrics', postgresql.JSONB))

def downgrade():
    op.drop_column('workflow_executions', 'ai_metrics')
    op.drop_table('ai_generation_mappings')
```

---

## Registro de Blueprint

- **Arquivo**: `app/__init__.py`
- Adicionar registro do novo blueprint

```python
def create_app(config_class=Config):
    # ... c√≥digo existente ...
    
    # Adicionar ap√≥s outras rotas:
    from app.routes import ai_routes
    app.register_blueprint(ai_routes.ai_bp)
    
    return app
```

---

## Tratamento de Erros e Fallback

### Estrat√©gia de Erro

```python
class AIGenerationError(Exception):
    """Erro gen√©rico de gera√ß√£o de IA"""
    
class AITimeoutError(AIGenerationError):
    """Timeout na chamada de IA"""
    
class AIQuotaExceededError(AIGenerationError):
    """Quota de API excedida"""
    
class AIInvalidKeyError(AIGenerationError):
    """API key inv√°lida"""

# No generator.py:
async def process_ai_tag(self, mapping, source_data):
    try:
        result = await self.llm_service.generate_text(
            model=f"{mapping.provider}/{mapping.model}",
            prompt=self.build_prompt(mapping, source_data),
            api_key=self.get_api_key(mapping.ai_connection),
            temperature=mapping.temperature,
            max_tokens=mapping.max_tokens,
            timeout=60  # 60 segundos timeout
        )
        return result.text
    except AITimeoutError:
        logger.warning(f"Timeout na gera√ß√£o AI para tag {mapping.ai_tag}")
        return mapping.fallback_value or "[Timeout na gera√ß√£o]"
    except AIQuotaExceededError:
        logger.error(f"Quota excedida para provider {mapping.provider}")
        raise  # Propagar para interromper documento
    except AIInvalidKeyError:
        logger.error(f"API key inv√°lida para connection {mapping.ai_connection_id}")
        raise  # Propagar para interromper documento
    except Exception as e:
        logger.error(f"Erro inesperado na gera√ß√£o AI: {e}")
        return mapping.fallback_value or f"[Erro: {mapping.ai_tag}]"
```

### Comportamento de Fallback

1. **Timeout**: Usa `fallback_value` se definido, sen√£o `[Timeout na gera√ß√£o]`
2. **Quota Excedida**: Interrompe documento, marca execution como `failed`
3. **API Key Inv√°lida**: Interrompe documento, marca execution como `failed`
4. **Outros Erros**: Usa `fallback_value` se definido, sen√£o `[Erro: tag_name]`

---

## Logging e Auditoria

### Configura√ß√£o de Logging

```python
# app/services/ai/llm_service.py
import logging

logger = logging.getLogger('docugen.ai')

class LLMService:
    async def generate_text(self, ...):
        logger.info(f"[AI] Iniciando gera√ß√£o - provider={provider}, model={model}")
        start_time = time.time()
        
        try:
            result = await litellm.acompletion(...)
            duration_ms = (time.time() - start_time) * 1000
            
            logger.info(
                f"[AI] Gera√ß√£o conclu√≠da - provider={provider}, model={model}, "
                f"tokens={result.usage.total_tokens}, time_ms={duration_ms:.0f}"
            )
            return result
            
        except Exception as e:
            logger.error(
                f"[AI] Erro na gera√ß√£o - provider={provider}, model={model}, "
                f"error={type(e).__name__}: {str(e)}"
            )
            raise
```

### Auditoria por Organiza√ß√£o

- Atualizar `usage_count` e `last_used_at` no mapping ap√≥s cada uso
- Salvar m√©tricas detalhadas em `WorkflowExecution.ai_metrics`
- Considerar tabela separada para hist√≥rico de uso (futuro)

---

## Processamento Ass√≠ncrono (Celery)

### Decis√£o: S√≠ncrono dentro do Fluxo Existente

O processamento de tags AI ser√° **s√≠ncrono** dentro da task Celery existente de gera√ß√£o de documentos, pois:

1. O documento j√° √© gerado em background via Celery
2. Tags AI s√£o parte do mesmo fluxo de gera√ß√£o
3. N√£o faz sentido criar sub-tasks para cada tag

### Timeout e Retry

```python
# Em tasks.py (se existir) ou generator.py
@celery.task(
    bind=True,
    max_retries=2,
    default_retry_delay=30,
    time_limit=300  # 5 minutos limite total
)
def generate_document_task(self, workflow_id, source_data):
    try:
        # ... gera√ß√£o com tags AI ...
    except AIQuotaExceededError as e:
        # N√£o retry para quota excedida
        raise
    except AITimeoutError as e:
        # Retry autom√°tico para timeout
        raise self.retry(exc=e)
```

---

## Fluxo de Execu√ß√£o Completo

1. Usu√°rio cria template com tag `{{ai:paragrapho1}}`
2. Usu√°rio configura conex√£o de IA (salva API key da OpenAI, por exemplo)
3. Usu√°rio configura workflow e cria mapeamento de IA:
   - Tag: `paragrapho1`
   - Campos: `dealname`, `amount`, `company.name`
   - Provider: `openai`, Model: `gpt-4`
   - Prompt: "Gere um par√°grafo descrevendo..."
   - Fallback: "[Texto n√£o dispon√≠vel]"
4. Ao gerar documento (manual ou via trigger):
   - Sistema detecta tag `{{ai:paragrapho1}}`
   - Busca mapeamento no workflow
   - Busca conex√£o do provedor configurado
   - Coleta valores dos campos HubSpot mapeados
   - Monta prompt com esses valores
   - Chama LiteLLM com timeout de 60s
   - Substitui tag pelo texto gerado (ou fallback)
   - Registra m√©tricas no execution
5. Continua processamento normal das outras tags
6. Salva documento final

---

## Arquivos a Criar

| Arquivo | Descri√ß√£o |
|---------|-----------|
| `app/services/ai/__init__.py` | Package init |
| `app/services/ai/llm_service.py` | Servi√ßo unificado com LiteLLM |
| `app/services/ai/utils.py` | Helpers e valida√ß√µes |
| `app/services/ai/exceptions.py` | Classes de exce√ß√£o customizadas |
| `app/routes/ai_routes.py` | Rotas auxiliares de IA |
| `migrations/versions/xxx_add_ai_generation_tables.py` | Migra√ß√£o de banco |

## Arquivos a Modificar

| Arquivo | Modifica√ß√£o |
|---------|-------------|
| `app/models/workflow.py` | Adicionar `AIGenerationMapping` e relationship |
| `app/models/__init__.py` | Exportar `AIGenerationMapping` |
| `app/models/execution.py` | Adicionar campo `ai_metrics` |
| `app/services/document_generation/tag_processor.py` | Detectar e processar tags AI |
| `app/services/document_generation/generator.py` | Integrar gera√ß√£o de IA |
| `app/routes/connections.py` | Endpoints para conex√µes de IA |
| `app/routes/workflows.py` | Endpoints CRUD para ai-mappings |
| `app/__init__.py` | Registrar blueprint `ai_routes` |
| `requirements.txt` | Adicionar `litellm>=1.50.0` |

---

## Depend√™ncias

```txt
# requirements.txt - Adicionar apenas:
litellm>=1.50.0
```

> ‚ö†Ô∏è **Nota**: N√ÉO adicionar `openai`, `google-generativeai`, `anthropic` individualmente. LiteLLM instala as depend√™ncias necess√°rias automaticamente quando o provedor √© usado.

---

## Ordem de Implementa√ß√£o

### Fase 1: Funda√ß√£o (3 tasks)
1. [x] Adicionar `litellm>=1.50.0` ao `requirements.txt`
2. [x] Criar migra√ß√£o para tabela `ai_generation_mappings` e coluna `ai_metrics`
3. [x] Criar modelo `AIGenerationMapping` e atualizar `Workflow` com relationship

### Fase 2: Servi√ßo de IA (2 tasks)
4. [x] Criar `app/services/ai/llm_service.py` com `LLMService`
5. [x] Criar `app/services/ai/utils.py` com helpers e `app/services/ai/exceptions.py`

### Fase 3: Processamento (2 tasks)
6. [x] Estender `TagProcessor` para detectar tags `{{ai:...}}`
7. [x] Atualizar `DocumentGenerator` para processar tags AI com m√©tricas

### Fase 4: Rotas (3 tasks)
8. [x] Adicionar endpoints de conex√µes de IA em `app/routes/connections.py` (incluindo `/test`)
9. [x] Adicionar endpoints CRUD para ai-mappings em `app/routes/workflows.py`
10. [x] Criar `app/routes/ai_routes.py` com rotas auxiliares (providers/models)

### Fase 5: Integra√ß√£o (2 tasks)
11. [x] Registrar blueprint `ai_routes` em `app/__init__.py`
12. [x] Atualizar `app/models/__init__.py` para exportar `AIGenerationMapping`

### Fase 6: Qualidade (2 tasks)
13. [x] Implementar logging estruturado em `LLMService`
14. [x] Criar testes unit√°rios e de integra√ß√£o

---

## Considera√ß√µes T√©cnicas

| Aspecto | Implementa√ß√£o |
|---------|---------------|
| **Arquitetura** | LiteLLM como √∫nica depend√™ncia de IA |
| **Seguran√ßa** | API keys criptografadas via `app/utils/encryption.py` |
| **Rate Limiting** | Usar retry logic embutido do LiteLLM |
| **Error Handling** | Exce√ß√µes customizadas + fallback por tag |
| **Custos** | M√©tricas de tokens salvos no execution |
| **Cache** | Considerar para v2 (prompt hash como chave) |
| **Timeout** | 60s por chamada, 300s limite total |
| **Logging** | Logger dedicado `docugen.ai` |
| **Auditoria** | `usage_count` + `ai_metrics` em execution |

---

## Status do Plano

üìã **Total de Tasks**: 14  
‚úÖ **Completadas**: 14/14 (100%)  
üéâ **Status**: IMPLEMENTA√á√ÉO CONCLU√çDA!

